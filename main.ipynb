{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!gshell init"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!gshell info --with-id 17axH-dYh-Wss_ec92ugSl8AmSuZS-xn9"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import signal\n",
    "from pathlib import Path\n",
    "import wget\n",
    "\n",
    "parser_version = '1.6.4'\n",
    "url = f'https://github.com/nemoware/document-parser/releases/download/{parser_version}/document-parser-{parser_version}.jar'\n",
    "if not Path(f'document-parser-{parser_version}.jar').is_file():\n",
    "    wget.download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(f'./{glob.glob(\"ДД по практикам*.zip\")[0]}',\n",
    "                     'r') as zip_ref:\n",
    "    zip_ref.extractall('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "key_value = ['о нижеследующем:', 'нижеследующем:', 'о нижеследующем', 'нижеследующем']\n",
    "for x in key_value[5:]:\n",
    "    print(123)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parser_version = '1.6.4'\n",
    "!java -cp \"document-parser-$parser_version/classes;document-parser-$parser_version/lib/*\" com.nemo.document.parser.App -i \"ДД по практикам\\Практика недропользования и экологии\\1. ДС N 8_испр.docx\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "s = [\n",
    "    \"gshell\",\n",
    "    \"init\"\n",
    "]\n",
    "\n",
    "subprocess.run(s, stdin=subprocess.PIPE, stdout=subprocess.PIPE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Start"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "import platform\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import fnmatch\n",
    "import base64\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "index = 1\n",
    "number_of_docs = 0\n",
    "parser_version = '1.6.4'\n",
    "root = \"ДД по практикам\"\n",
    "\n",
    "with_neuron = False\n",
    "\n",
    "tokenizer_path = \"sberbank-ai/ruRoberta-large\"\n",
    "path_to_model = \"./doc-classification/\"\n",
    "\n",
    "labels = ['Практика коммерческой логистики',\n",
    "          'Практика недропользования и экологии',\n",
    "          'Практика поддержки региональных, розничных продаж и клиентского сервиса',\n",
    "          'Практика правового сопровождения закупок МТР и услуг общего профиля',\n",
    "          'Практика правового сопровождения земельных отношений и сделок с недвижимым имуществом',\n",
    "          'Практика правового сопровождения операционной деятельности БРД',\n",
    "          'Практика правового сопровождения переработки и инфраструктуры',\n",
    "          'Практика правовой поддержки брендов',\n",
    "          'Практика правовой поддержки использования и коммерциализации ИС',\n",
    "          'Практика правовой поддержки создания и приобретения ИС',\n",
    "          'Практика промышленной безопасности и охраны труда',\n",
    "          'Практика финансового и конкурентного права',\n",
    "          'Практика экспорта, оптовых продаж и сбыта бизнес-единиц (БЕ)']\n",
    "\n",
    "arrOfAllDocs = []\n",
    "result = []\n",
    "result_of_fail = []\n",
    "result_of_possible = []\n",
    "result_of_possible2 = []\n",
    "\n",
    "s = [\n",
    "    \"java\",\n",
    "    \"-jar\",\n",
    "    f\"document-parser-{parser_version}.jar\",\n",
    "    \"--server.port=8083\"\n",
    "]\n",
    "headers = {\n",
    "    'Content-type': 'application/json',\n",
    "    'Accept': 'application/json; text/plain'\n",
    "}\n",
    "\n",
    "for root, dirnames, filenames in os.walk('ДД по практикам'):\n",
    "    if len(root.split('\\\\')) == 1: continue\n",
    "    flag = False\n",
    "    for i in root.split('\\\\'):\n",
    "        if str(i).startswith('Исключена'):\n",
    "            flag = True\n",
    "            break\n",
    "    if flag: continue\n",
    "    for filename in fnmatch.filter(filenames, '*.docx'):\n",
    "        arrOfAllDocs.append(os.path.join(root, filename))\n",
    "    for filename in fnmatch.filter(filenames, '*.doc'):\n",
    "        arrOfAllDocs.append(os.path.join(root, filename))\n",
    "\n",
    "print(\n",
    "    \"Запуск document-parser на 8083 порту, если что-то пойдет не так, то руками УБЕЙТЕ java процесс\"\n",
    ")\n",
    "java_subprocess = subprocess.Popen(s, creationflags=subprocess.CREATE_NEW_PROCESS_GROUP,\n",
    "                                   stdout=subprocess.PIPE, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "def find_let(document, documentType=None):\n",
    "    key_value = ['о нижеследующем:', 'нижеследующем:', 'о нижеследующем', 'нижеследующем']\n",
    "    if documentType == 'SUPPLEMENTARY_AGREEMENT':\n",
    "        key_value.append('в следующей редакции:')\n",
    "        key_value.append('заключили настоящее Дополнительное соглашение к Договору.')\n",
    "        key_value.append('редакции:')\n",
    "    result = \"\"\n",
    "    for i, p in enumerate(document['paragraphs']):\n",
    "        if any(f.lower() in p['paragraphBody']['text'].lower() or f.lower() in\n",
    "               p['paragraphHeader']['text'].lower() for f in\n",
    "               key_value[:4]) or any(f.lower() in p['paragraphBody']['text'].lower() or f.lower() in\n",
    "                                     p['paragraphHeader']['text'].lower() for f in\n",
    "                                     key_value[5:]):\n",
    "            text = \"\"\n",
    "            for x in key_value[:4]:\n",
    "                if x.lower() in p['paragraphBody']['text'].lower():\n",
    "                    text += p['paragraphBody']['text'].split(x)[1]\n",
    "                    break\n",
    "                if x.lower() in p['paragraphHeader']['text'].lower():\n",
    "                    text += p['paragraphBody']['text']\n",
    "                    break\n",
    "\n",
    "            if text == \"\":\n",
    "                for x in key_value[5:]:\n",
    "                    if x.lower() in p['paragraphBody']['text'].lower():\n",
    "                        text += p['paragraphBody']['text'].split(x)[1]\n",
    "                        break\n",
    "                    if x.lower() in p['paragraphHeader']['text'].lower():\n",
    "                        text += p['paragraphBody']['text']\n",
    "                        break\n",
    "\n",
    "            if text == \"\": continue\n",
    "\n",
    "            text += \"\".join(\n",
    "                str(x['paragraphBody']['text']) for x in document['paragraphs'][i + 1:i + 4])\n",
    "\n",
    "            textHeader = p['paragraphHeader']['text'] + \"\\n\".join(\n",
    "                str(x['paragraphHeader']['text']) for x in document['paragraphs'][i:i + 4])\n",
    "\n",
    "            d = i + 4\n",
    "            while len(text.split()) < 300 and d < len(document['paragraphs']):\n",
    "                text += document['paragraphs'][d]['paragraphBody']['text']\n",
    "                d += 1\n",
    "\n",
    "            result = {\n",
    "                \"path\": docs,\n",
    "                \"name\": docs.split(\"\\\\\")[-1],\n",
    "                \"documentType\": document['documentType'],\n",
    "                \"offset\": p['paragraphBody']['offset'],\n",
    "                \"text\": text,\n",
    "                \"length\": len(text),\n",
    "                \"offsetHeader\": p['paragraphHeader']['offset'],\n",
    "                \"textHeader\": textHeader,\n",
    "                \"lengthHeader\": len(textHeader),\n",
    "                \"isTrue\": is_true(text, docs)\n",
    "            }\n",
    "            break\n",
    "    return result\n",
    "\n",
    "\n",
    "def find_currency_header(paragraph, keys):\n",
    "    paragraph['paragraphHeader']['text'] = re.sub(' +', ' ', paragraph['paragraphHeader']['text'])\n",
    "    header_text_in_low_reg = paragraph['paragraphHeader']['text'].lower()\n",
    "    basic_text_in_low_reg = paragraph['paragraphBody']['text'].lower()\n",
    "    if paragraph['paragraphBody']['length'] < 20:\n",
    "        return False\n",
    "    if len(basic_text_in_low_reg.split()) < 15:\n",
    "        return False\n",
    "    if re.search(\"(:)\\s*$\", basic_text_in_low_reg):\n",
    "        return False\n",
    "\n",
    "    for key in keys:\n",
    "        if key.lower() in header_text_in_low_reg:\n",
    "            if 'Статья'.lower() in key.lower() and any(\n",
    "                    x.lower() in header_text_in_low_reg for x in\n",
    "                    ['Термины и определения', 'Термин', 'определения']):\n",
    "                return False\n",
    "\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def get_tokens(text):\n",
    "    result = tokenizer(text, truncation=True, max_length=512)\n",
    "    return result\n",
    "\n",
    "\n",
    "def predicate_result(text):\n",
    "    tokens = get_tokens(text)\n",
    "    predictions = model.predict([tokens['input_ids']])['logits']\n",
    "    predictions = tf.nn.softmax(predictions, name=None)[0].numpy()\n",
    "    print(predictions)\n",
    "    print(sum(predictions))\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def is_true(text, path):\n",
    "    if with_neuron: return False\n",
    "    result = predicate_result(text)\n",
    "    argmax = np.argmax(result, axis=0)\n",
    "    required_label = labels[argmax]\n",
    "    if required_label in path:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "time.sleep(2)\n",
    "i = 1\n",
    "while True:\n",
    "    time.sleep(0.1)\n",
    "    output_log_spring = java_subprocess.stdout.readline()\n",
    "    sys.stdout.write(\"\\rПроверка соединения #%i\" % i)\n",
    "    sys.stdout.flush()\n",
    "    i += 1\n",
    "    if output_log_spring.find(\"Started DocumentParserService\") != -1:\n",
    "        print(\"\\nГотово\")\n",
    "        java_subprocess.stdout.close()\n",
    "        break\n",
    "\n",
    "print(\"Запустился успешно\")\n",
    "print(\"Общее количество документов =\", len(arrOfAllDocs))\n",
    "\n",
    "tokenizer = None\n",
    "model = None\n",
    "\n",
    "if with_neuron:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(str(tokenizer_path))\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "        str(path_to_model), num_labels=len(labels), from_pt=False\n",
    "    )\n",
    "\n",
    "for docs in arrOfAllDocs:\n",
    "    try:\n",
    "        file = open(docs, 'rb')\n",
    "        encoded_string = base64.b64encode(file.read())\n",
    "        encoded_string = str(encoded_string)[2:-1]\n",
    "    except Exception as e:\n",
    "        # print(f\"\\nОшибка в файле {docs}\")\n",
    "        # print(f\"при конвертации в base64, исключение = {e.msg}\")\n",
    "        # print(\"=\" * 200)\n",
    "        continue\n",
    "\n",
    "    response = requests.post(\n",
    "        \"http://localhost:8083/document-parser\",\n",
    "        data=json.dumps({\n",
    "            \"base64Content\": encoded_string,\n",
    "            \"documentFileType\": docs.split(\".\")[-1].upper()\n",
    "        }),\n",
    "        headers=headers\n",
    "    )\n",
    "    resArr = []\n",
    "    try:\n",
    "        resArr = response.json()['documents']\n",
    "    except Exception as e:\n",
    "        # print(f\"\\nОшибка в файле {docs}\")\n",
    "        # print(f\"Ответ от парсера {response.json()}\")\n",
    "        # print(f\"Исключение = {e}\")\n",
    "        # print(\"=\" * 200)\n",
    "        continue\n",
    "\n",
    "    sys.stdout.write(f\"\\rПроверка документа под номером {index} и ИД {len(result)}\")\n",
    "    sys.stdout.flush()\n",
    "    index += 1\n",
    "\n",
    "    document = []\n",
    "    if resArr:\n",
    "        document = resArr[0]\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    if document['documentType'] == \"CONTRACT\" or document['documentType'] == \"AGREEMENT\":\n",
    "        for ind, par in enumerate(document['paragraphs']):\n",
    "            document['paragraphs'][ind]['paragraphBody']['text'] = re.sub('_+', '',\n",
    "                                                                          par['paragraphBody'][\n",
    "                                                                              'text'])\n",
    "\n",
    "        keys = ['Общие ', 'Общие сведения', 'Общие положение', 'Общие условия', 'Статья 1']\n",
    "\n",
    "        if document['documentType'] == \"CONTRACT\":\n",
    "            sup_keys = ['предмет договра', 'предмет договора', 'Предмет контракта', 'Предмет догов',\n",
    "                        'Предмет и общие условия договора']\n",
    "            keys = sup_keys + keys\n",
    "        if document['documentType'] == \"AGREEMENT\":\n",
    "            keys.append('Предмет соглашения')\n",
    "\n",
    "        flag = False\n",
    "        for i, p in enumerate(document['paragraphs']):\n",
    "            if find_currency_header(paragraph=p, keys=keys):\n",
    "                result.append(\n",
    "                    {\n",
    "                        \"path\": docs,\n",
    "                        \"name\": docs.split(\"\\\\\")[-1],\n",
    "                        \"documentType\": document['documentType'],\n",
    "                        \"offset\": p['paragraphBody']['offset'],\n",
    "                        \"text\": p['paragraphBody']['text'],\n",
    "                        \"length\": p['paragraphBody']['length'],\n",
    "                        \"offsetHeader\": p['paragraphHeader']['offset'],\n",
    "                        \"textHeader\": p['paragraphHeader']['text'],\n",
    "                        \"lengthHeader\": p['paragraphHeader']['length'],\n",
    "                        \"isTrue\": is_true(p['paragraphBody']['text'], docs)\n",
    "                    })\n",
    "                flag = True\n",
    "                break\n",
    "        if flag: continue\n",
    "\n",
    "        all_text = \"\".join(\n",
    "            str('\\n' + x['paragraphHeader']['text'] + '\\n' + x['paragraphBody']['text']) for x in\n",
    "            document['paragraphs'])\n",
    "        all_text = re.sub(' +', ' ', all_text)\n",
    "        text_from = \"\"\n",
    "\n",
    "        for key in keys:\n",
    "            if key.lower() in all_text.lower():\n",
    "                array_of_text = re.split(f\"(?i)({key})\", all_text)\n",
    "                end_text = 0\n",
    "                try:\n",
    "                    last_symbol = re.search(\"\\s\\d[ .]\\d?[\\s|\\u00A0|.\\s]*$\", array_of_text[0])\n",
    "                    if last_symbol:\n",
    "                        end_text = int(last_symbol.group().replace(\" \", \"\").split(\".\")[0])\n",
    "                    else:\n",
    "                        text_from = \" \".join(array_of_text[2].split()[:300])\n",
    "                        # print(\"end_text =\",end_text)\n",
    "                except ValueError as ex:\n",
    "                    print(f\"cannot converted str to int\")\n",
    "                    text_from = \" \".join(array_of_text[2].split()[:300])\n",
    "                    break\n",
    "\n",
    "                if end_text:\n",
    "                    end_text += 1\n",
    "                    print(\"\\nEnd = \", end_text)\n",
    "                    text_from = re.split(f\"\\s({end_text})[. ]\", array_of_text[2])[0]\n",
    "                    break\n",
    "        if text_from != \"\":\n",
    "            result.append({\n",
    "                \"path\": docs,\n",
    "                \"name\": docs.split(\"\\\\\")[-1],\n",
    "                \"documentType\": document['documentType'],\n",
    "                \"offset\": document['paragraphs'][0]['paragraphBody']['offset'],\n",
    "                \"text\": text_from,\n",
    "                \"length\": len(text_from),\n",
    "                \"offsetHeader\": p['paragraphHeader']['offset'],\n",
    "                \"textHeader\": p['paragraphHeader']['text'],\n",
    "                \"lengthHeader\": p['paragraphHeader']['length'],\n",
    "                \"isTrue\": is_true(p['paragraphBody']['text'], docs)\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        obj = find_let(document)\n",
    "        if obj != \"\":\n",
    "            result.append(obj)\n",
    "            flag = True\n",
    "\n",
    "        if flag: continue\n",
    "        result_of_fail.append(\n",
    "            {\n",
    "                \"path\": docs,\n",
    "                \"name\": docs.split(\"\\\\\")[-1],\n",
    "                \"documentType\": document['documentType'],\n",
    "                \"offset\": document['paragraphs'][0]['paragraphBody']['offset'],\n",
    "                \"text\": \"\\n+++++++++++++\\n\".join(\n",
    "                    str(x['paragraphBody']['text']) for x in document['paragraphs']),\n",
    "                \"length\": sum(i['paragraphBody']['length'] for i in document['paragraphs']),\n",
    "                \"offsetHeader\": document['paragraphs'][0]['paragraphHeader']['offset'],\n",
    "                \"textHeader\": \"\\n+++++++++++++\\n\".join(\n",
    "                    str(x['paragraphHeader']['text']) for x in document['paragraphs']),\n",
    "                \"lengthHeader\": sum(i['paragraphHeader']['length'] for i in document['paragraphs'])\n",
    "            })\n",
    "    elif document['documentType'] == \"SUPPLEMENTARY_AGREEMENT\":\n",
    "        for ind, par in enumerate(document['paragraphs']):\n",
    "            document['paragraphs'][ind]['paragraphBody']['text'] = re.sub('_+', '',\n",
    "                                                                          par['paragraphBody'][\n",
    "                                                                              'text'])\n",
    "        flag = False\n",
    "        for i, p in enumerate(document['paragraphs']):\n",
    "            if any(f.lower() in p['paragraphHeader']['text'].lower() for f in\n",
    "                   ['Статья 1']) and p['paragraphBody'][\n",
    "                'length'] > 20:\n",
    "                result.append(\n",
    "                    {\n",
    "                        \"path\": docs,\n",
    "                        \"name\": docs.split(\"\\\\\")[-1],\n",
    "                        \"documentType\": document['documentType'],\n",
    "                        \"offset\": p['paragraphBody']['offset'],\n",
    "                        \"text\": p['paragraphBody']['text'],\n",
    "                        \"length\": p['paragraphBody']['length'],\n",
    "                        \"offsetHeader\": p['paragraphHeader']['offset'],\n",
    "                        \"textHeader\": p['paragraphHeader']['text'],\n",
    "                        \"lengthHeader\": p['paragraphHeader']['length'],\n",
    "                        \"isTrue\": is_true(p['paragraphBody']['text'], docs)\n",
    "                    })\n",
    "                flag = True\n",
    "                break\n",
    "        if flag: continue\n",
    "        obj = find_let(document, document['documentType'])\n",
    "        if obj != \"\":\n",
    "            result.append(obj)\n",
    "            flag = True\n",
    "\n",
    "        if flag: continue\n",
    "        #document['paragraphs'][0]['paragraphHeader']['text']\n",
    "        result.append(\n",
    "            {\n",
    "                \"path\": docs,\n",
    "                \"name\": docs.split(\"\\\\\")[-1],\n",
    "                \"documentType\": document['documentType'],\n",
    "                \"offset\": document['paragraphs'][0]['paragraphBody']['offset'],\n",
    "                \"text\": \"\".join(\n",
    "                    str(x['paragraphBody']['text']) for x in document['paragraphs']),\n",
    "                \"length\": sum(i['paragraphBody']['length'] for i in document['paragraphs']),\n",
    "                \"offsetHeader\": document['paragraphs'][0]['paragraphHeader']['offset'],\n",
    "                \"textHeader\": \"\\n+++++++++++++\\n\".join(\n",
    "                    str(x['paragraphHeader']['text']) for x in document['paragraphs']),\n",
    "                \"lengthHeader\": sum(i['paragraphHeader']['length'] for i in document['paragraphs']),\n",
    "                \"isTrue\": is_true(\"\".join(\n",
    "                    str(x['paragraphBody']['text']) for x in document['paragraphs']), docs)\n",
    "            })\n",
    "\n",
    "#Смерть java процессу!\n",
    "if platform.system() == 'Windows':\n",
    "    subprocess.run(\"TASKKILL /F /PID {pid} /T\".format(pid=java_subprocess.pid))\n",
    "elif platform.system() == 'Linux':\n",
    "    os.kill(java_subprocess.pid, signal.SIGTERM)\n",
    "else:\n",
    "    print('Не известная платформа, убейте в ручную процесс java')\n",
    "# os.killpg(os.getpgid(java_subprocess.pid), signal.SIGTERM)\n",
    "\n",
    "writer = pd.ExcelWriter(\"classifier.xlsx\", engine='xlsxwriter')\n",
    "\n",
    "df = pd.DataFrame(result)\n",
    "df.to_excel(writer, 'good', engine='xlsxwriter')\n",
    "sheets_good = writer.sheets['good']\n",
    "sheets_good.autofilter(0, 0, df.shape[0], df.shape[1])\n",
    "\n",
    "df = pd.DataFrame(result_of_fail)\n",
    "df.to_excel(writer, 'SO SO', engine='xlsxwriter')\n",
    "sheets_bad = writer.sheets['SO SO']\n",
    "sheets_bad.autofilter(0, 0, df.shape[0], df.shape[1])\n",
    "\n",
    "# df = pd.DataFrame(result_of_possible)\n",
    "# df.to_excel(writer, 'SO SO SO', engine='xlsxwriter')\n",
    "# sheets_bad = writer.sheets['SO SO SO']\n",
    "# sheets_bad.autofilter(0, 0, df.shape[0], df.shape[1])\n",
    "#\n",
    "# df = pd.DataFrame(result_of_possible2)\n",
    "# df.to_excel(writer, 'О ниже', engine='xlsxwriter')\n",
    "# sheets_bad = writer.sheets['О ниже']\n",
    "# sheets_bad.autofilter(0, 0, df.shape[0], df.shape[1])\n",
    "\n",
    "writer.save()\n",
    "print(\"\\nФайл создан\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Копия блокнота \"Test document-parser.ipynb\"",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}